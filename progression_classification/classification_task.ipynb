{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, label_binarize, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import matthews_corrcoef, accuracy_score, make_scorer\n",
    "from tqdm.notebook import tqdm, tnrange\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.distributions as dist\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "import gpytorch\n",
    "\n",
    "from classification_utils import *\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_state = 1 # random seed used in every place which take a seed as input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label categories:\n",
    "* 0 corresponds to 'No trend' \n",
    "* 1 corresponds to 'Decreasing trend' \n",
    "* 2 corresponds to 'Increasing trend'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nick_\\Anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "c:\\Users\\nick_\\Anaconda3\\envs\\thesis\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('cross_sectional_data.csv', index_col=[0])\n",
    "X , y = encode(data.drop(columns=['ID', 'Labels'])) , data['Labels'].to_numpy()\n",
    "# Fix missing values\n",
    "col_list = list(X.columns.values)\n",
    "X = X.reindex(columns=col_list).fillna(0)\n",
    "X = X[col_list]\n",
    "X_numeric = X.iloc[: , :8]\n",
    "\n",
    "# Standardize numeric features\n",
    "scaler = StandardScaler()\n",
    "X_numeric = scaler.fit_transform(X_numeric)\n",
    "X_numeric = pd.DataFrame(X_numeric, columns=encode(data.drop(columns=['ID', 'Labels'])).columns.values[:8])\n",
    "X = pd.concat([X_numeric, X.iloc[:, 8:]], axis=1)\n",
    "X = X.drop(columns=['r2', 'slope']) # remove these two features since they won't be available in case of model deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data into training and test sets\n",
    "\n",
    "We then fine-tune the models using sklearn functionality on the training set, so for now we don't create any validation sets.\n",
    "\n",
    "We are also careful to keep the proportions of labels in the training, testing and validation sets the same. This is quite important since we are dealing with imbalanced labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.20\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size, random_state = rand_state, stratify = y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 3}\n",
      "MCC on test set using Logistic regression: 0.109\n",
      "Accuracy on test set using Logistic regression: 0.686\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=rand_state) # can use skf.split(X_train, y_train) to get the indices of the folding (useful when working with BNN and GP)\n",
    "\n",
    "\n",
    "lr_C_vals = [0.01, 0.1, 0.5, 1, 3, 5, 10, 20, 30, 40, 50, 70, 100, 200, 500, 1000, 2000, 5000, 10_000, 100_000] # L2-regularisation values\n",
    "log_reg = LogisticRegression(penalty='l2', max_iter=10_000, multi_class='multinomial')\n",
    "log_reg_grid = {'C' : lr_C_vals}\n",
    "# Tune model\n",
    "tuned_log_reg , log_reg_test_mcc = tune_model_and_get_test_mcc(X, y, log_reg, rand_state=1, hyperparam_grid=log_reg_grid, test_size=0.2, print_best_params=True)\n",
    "# Evaluate on test set\n",
    "print(f'MCC on test set using Logistic regression: {log_reg_test_mcc :.3f}')\n",
    "print(f'Accuracy on test set using Logistic regression: {accuracy_score(y_test , tuned_log_reg.predict(X_test)) :.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support vector classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 200, 'gamma': 0.01}\n",
      "MCC on test set using SVC: 0.282\n",
      "Accuracy on test set using SVC: 0.736\n"
     ]
    }
   ],
   "source": [
    "svm_C_vals = [0.01, 0.1, 0.5, 1, 3, 5, 10, 20, 30, 40, 50, 70, 100, 200, 500, 1000, 2000, 5000, 10_000, 100_000] # regularisation strength constant\n",
    "rbf_gamma_vals = [0.0001, 0.001, 0.005, 0.01, 0.1, 0.2, 0.5, 0.8, 1, 2, 3, 5, 10, 15, 30, 40, 50, 100, 200, 500, 1000, 10_000]\n",
    "svc_grid = {'C' : svm_C_vals, 'gamma' : rbf_gamma_vals}\n",
    "svc = SVC(kernel='rbf')\n",
    "\n",
    "# Tune model\n",
    "tuned_svc , svc_test_mcc = tune_model_and_get_test_mcc(X, y, svc, rand_state=1, hyperparam_grid=svc_grid, test_size=0.2, print_best_params=True)\n",
    "# Evaluate on test set\n",
    "print(f'MCC on test set using SVC: {svc_test_mcc :.3f}')\n",
    "print(f'Accuracy on test set using SVC: {accuracy_score(y_test , tuned_svc.predict(X_test)) :.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest classifier (tuning takes 80 mins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 30, 'max_features': 'auto', 'min_samples_split': 10, 'n_estimators': 400}\n",
      "MCC on test set using RF: 0.204\n",
      "Accuracy on test set using RF: 0.723\n"
     ]
    }
   ],
   "source": [
    "rf_n_est = [100, 200, 300, 400, 500, 1000, 1500, 2000, 5000, 10_000]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "rf_grid = {'n_estimators' : rf_n_est, 'max_depth' : max_depth, 'max_features' : max_features, 'min_samples_split' : min_samples_split}\n",
    "rf = RandomForestClassifier(n_estimators=100, criterion='gini', random_state=1, n_jobs=-1)\n",
    "\n",
    "# Tune model\n",
    "tuned_rf , rf_test_mcc = tune_model_and_get_test_mcc(X, y, rf, rand_state=1, hyperparam_grid=rf_grid, test_size=0.2, print_best_params=True)\n",
    "# Evaluate on test set\n",
    "print(f'MCC on test set using RF: {rf_test_mcc :.3f}')\n",
    "print(f'Accuracy on test set using RF: {accuracy_score(y_test , tuned_rf.predict(X_test)) :.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing around with `GPytorch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.from_numpy(X_train.to_numpy().astype(np.float32))\n",
    "train_y = torch.from_numpy(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([460, 29])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.models import AbstractVariationalGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution\n",
    "from gpytorch.variational import VariationalStrategy\n",
    "from gpytorch.mlls.variational_elbo import VariationalELBO\n",
    "\n",
    "\n",
    "class GPClassificationModel(AbstractVariationalGP):\n",
    "    \n",
    "    def __init__(self, train_x):\n",
    "        variational_distribution = CholeskyVariationalDistribution(train_x.size(0))\n",
    "        variational_strategy = VariationalStrategy(self, train_x, variational_distribution)\n",
    "        super(GPClassificationModel, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        latent_pred = gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "        return latent_pred\n",
    "\n",
    "\n",
    "# Initialize model and likelihood\n",
    "model = GPClassificationModel(train_x)\n",
    "likelihood = gpytorch.likelihoods.SoftmaxLikelihood(num_classes=3, mixing_weights=False, num_features=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultivariateNormal(loc: torch.Size([460]))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "There should be 3 features",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\nick_\\OneDrive\\Desktop\\Thesis code\\my_work\\progression_classification\\classification_model.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nick_/OneDrive/Desktop/Thesis%20code/my_work/progression_classification/classification_model.ipynb#ch0000007?line=17'>18</a>\u001b[0m \u001b[39mprint\u001b[39m(output)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nick_/OneDrive/Desktop/Thesis%20code/my_work/progression_classification/classification_model.ipynb#ch0000007?line=18'>19</a>\u001b[0m \u001b[39m# Calc loss and backprop gradients\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/nick_/OneDrive/Desktop/Thesis%20code/my_work/progression_classification/classification_model.ipynb#ch0000007?line=19'>20</a>\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mmll(output, train_y)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nick_/OneDrive/Desktop/Thesis%20code/my_work/progression_classification/classification_model.ipynb#ch0000007?line=20'>21</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nick_/OneDrive/Desktop/Thesis%20code/my_work/progression_classification/classification_model.ipynb#ch0000007?line=21'>22</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mIter \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m - Loss: \u001b[39m\u001b[39m%.3f\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, training_iter, loss\u001b[39m.\u001b[39mitem()))\n",
      "File \u001b[1;32mc:\\Users\\nick_\\Anaconda3\\envs\\thesis\\lib\\site-packages\\gpytorch\\module.py:30\u001b[0m, in \u001b[0;36mModule.__call__\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/nick_/Anaconda3/envs/thesis/lib/site-packages/gpytorch/module.py?line=28'>29</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m---> <a href='file:///c%3A/Users/nick_/Anaconda3/envs/thesis/lib/site-packages/gpytorch/module.py?line=29'>30</a>\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     <a href='file:///c%3A/Users/nick_/Anaconda3/envs/thesis/lib/site-packages/gpytorch/module.py?line=30'>31</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(outputs, \u001b[39mlist\u001b[39m):\n\u001b[0;32m     <a href='file:///c%3A/Users/nick_/Anaconda3/envs/thesis/lib/site-packages/gpytorch/module.py?line=31'>32</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[39mfor\u001b[39;00m output \u001b[39min\u001b[39;00m outputs]\n",
      "File \u001b[1;32mc:\\Users\\nick_\\Anaconda3\\envs\\thesis\\lib\\site-packages\\gpytorch\\mlls\\variational_elbo.py:77\u001b[0m, in \u001b[0;36mVariationalELBO.forward\u001b[1;34m(self, variational_dist_f, target, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/nick_/Anaconda3/envs/thesis/lib/site-packages/gpytorch/mlls/variational_elbo.py?line=62'>63</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, variational_dist_f, target, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     <a href='file:///c%3A/Users/nick_/Anaconda3/envs/thesis/lib/site-packages/gpytorch/mlls/variational_elbo.py?line=63'>64</a>\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/nick_/Anaconda3/envs/thesis/lib/site-packages/gpytorch/mlls/variational_elbo.py?line=64'>65</a>\u001b[0m \u001b[39m    Computes the Variational ELBO given :math:`q(\\mathbf f)` and :math:`\\mathbf y`.\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/nick_/Anaconda3/envs/thesis/lib/site-packages/gpytorch/mlls/variational_elbo.py?line=65'>66</a>\u001b[0m \u001b[39m    Calling this function will call the likelihood's :meth:`~gpytorch.likelihoods.Likelihood.expected_log_prob`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/nick_/Anaconda3/envs/thesis/lib/site-packages/gpytorch/mlls/variational_elbo.py?line=74'>75</a>\u001b[0m \u001b[39m    :return: Variational ELBO. Output shape corresponds to batch shape of the model/input data.\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/nick_/Anaconda3/envs/thesis/lib/site-packages/gpytorch/mlls/variational_elbo.py?line=75'>76</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> <a href='file:///c%3A/Users/nick_/Anaconda3/envs/thesis/lib/site-packages/gpytorch/mlls/variational_elbo.py?line=76'>77</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mforward(variational_dist_f, target, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nick_\\Anaconda3\\envs\\thesis\\lib\\site-packages\\gpytorch\\mlls\\_approximate_mll.py:57\u001b[0m, in \u001b[0;36m_ApproximateMarginalLogLikelihood.forward\u001b[1;34m(self, approximate_dist_f, target, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/nick_/Anaconda3/envs/thesis/lib/site-packages/gpytorch/mlls/_approximate_mll.py?line=54'>55</a>\u001b[0m \u001b[39m# Get likelihood term and KL term\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/nick_/Anaconda3/envs/thesis/lib/site-packages/gpytorch/mlls/_approximate_mll.py?line=55'>56</a>\u001b[0m num_batch \u001b[39m=\u001b[39m approximate_dist_f\u001b[39m.\u001b[39mevent_shape[\u001b[39m0\u001b[39m]\n\u001b[1;32m---> <a href='file:///c%3A/Users/nick_/Anaconda3/envs/thesis/lib/site-packages/gpytorch/mlls/_approximate_mll.py?line=56'>57</a>\u001b[0m log_likelihood \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_log_likelihood_term(approximate_dist_f, target, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\u001b[39m.\u001b[39mdiv(num_batch)\n\u001b[0;32m     <a href='file:///c%3A/Users/nick_/Anaconda3/envs/thesis/lib/site-packages/gpytorch/mlls/_approximate_mll.py?line=57'>58</a>\u001b[0m kl_divergence \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mvariational_strategy\u001b[39m.\u001b[39mkl_divergence()\u001b[39m.\u001b[39mdiv(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_data \u001b[39m/\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbeta)\n\u001b[0;32m     <a href='file:///c%3A/Users/nick_/Anaconda3/envs/thesis/lib/site-packages/gpytorch/mlls/_approximate_mll.py?line=59'>60</a>\u001b[0m \u001b[39m# Add any additional registered loss terms\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nick_\\Anaconda3\\envs\\thesis\\lib\\site-packages\\gpytorch\\mlls\\variational_elbo.py:61\u001b[0m, in \u001b[0;36mVariationalELBO._log_likelihood_term\u001b[1;34m(self, variational_dist_f, target, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/nick_/Anaconda3/envs/thesis/lib/site-packages/gpytorch/mlls/variational_elbo.py?line=59'>60</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_log_likelihood_term\u001b[39m(\u001b[39mself\u001b[39m, variational_dist_f, target, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m---> <a href='file:///c%3A/Users/nick_/Anaconda3/envs/thesis/lib/site-packages/gpytorch/mlls/variational_elbo.py?line=60'>61</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlikelihood\u001b[39m.\u001b[39;49mexpected_log_prob(target, variational_dist_f, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\u001b[39m.\u001b[39msum(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\nick_\\Anaconda3\\envs\\thesis\\lib\\site-packages\\gpytorch\\likelihoods\\likelihood.py:179\u001b[0m, in \u001b[0;36mLikelihood.expected_log_prob\u001b[1;34m(self, observations, function_dist, *args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/nick_/Anaconda3/envs/thesis/lib/site-packages/gpytorch/likelihoods/likelihood.py?line=162'>163</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexpected_log_prob\u001b[39m(\u001b[39mself\u001b[39m, observations, function_dist, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    <a href='file:///c%3A/Users/nick_/Anaconda3/envs/thesis/lib/site-packages/gpytorch/likelihoods/likelihood.py?line=163'>164</a>\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/nick_/Anaconda3/envs/thesis/lib/site-packages/gpytorch/likelihoods/likelihood.py?line=164'>165</a>\u001b[0m \u001b[39m    (Used by :obj:`~gpytorch.mlls.VariationalELBO` for variational inference.)\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/nick_/Anaconda3/envs/thesis/lib/site-packages/gpytorch/likelihoods/likelihood.py?line=165'>166</a>\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/nick_/Anaconda3/envs/thesis/lib/site-packages/gpytorch/likelihoods/likelihood.py?line=176'>177</a>\u001b[0m \u001b[39m    :rtype: torch.Tensor\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/nick_/Anaconda3/envs/thesis/lib/site-packages/gpytorch/likelihoods/likelihood.py?line=177'>178</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/nick_/Anaconda3/envs/thesis/lib/site-packages/gpytorch/likelihoods/likelihood.py?line=178'>179</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mexpected_log_prob(observations, function_dist, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nick_\\Anaconda3\\envs\\thesis\\lib\\site-packages\\gpytorch\\likelihoods\\likelihood.py:38\u001b[0m, in \u001b[0;36m_Likelihood.expected_log_prob\u001b[1;34m(self, observations, function_dist, *args, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/nick_/Anaconda3/envs/thesis/lib/site-packages/gpytorch/likelihoods/likelihood.py?line=36'>37</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mexpected_log_prob\u001b[39m(\u001b[39mself\u001b[39m, observations, function_dist, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m---> <a href='file:///c%3A/Users/nick_/Anaconda3/envs/thesis/lib/site-packages/gpytorch/likelihoods/likelihood.py?line=37'>38</a>\u001b[0m     likelihood_samples \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_draw_likelihood_samples(function_dist, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     <a href='file:///c%3A/Users/nick_/Anaconda3/envs/thesis/lib/site-packages/gpytorch/likelihoods/likelihood.py?line=38'>39</a>\u001b[0m     res \u001b[39m=\u001b[39m likelihood_samples\u001b[39m.\u001b[39mlog_prob(observations)\u001b[39m.\u001b[39mmean(dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m     <a href='file:///c%3A/Users/nick_/Anaconda3/envs/thesis/lib/site-packages/gpytorch/likelihoods/likelihood.py?line=39'>40</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m res\n",
      "File \u001b[1;32mc:\\Users\\nick_\\Anaconda3\\envs\\thesis\\lib\\site-packages\\gpytorch\\likelihoods\\likelihood.py:161\u001b[0m, in \u001b[0;36mLikelihood._draw_likelihood_samples\u001b[1;34m(self, function_dist, sample_shape, *args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/nick_/Anaconda3/envs/thesis/lib/site-packages/gpytorch/likelihoods/likelihood.py?line=158'>159</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[0;32m    <a href='file:///c%3A/Users/nick_/Anaconda3/envs/thesis/lib/site-packages/gpytorch/likelihoods/likelihood.py?line=159'>160</a>\u001b[0m     function_samples \u001b[39m=\u001b[39m function_samples\u001b[39m.\u001b[39msqueeze(\u001b[39m-\u001b[39m\u001b[39mlen\u001b[39m(function_dist\u001b[39m.\u001b[39mevent_shape) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m--> <a href='file:///c%3A/Users/nick_/Anaconda3/envs/thesis/lib/site-packages/gpytorch/likelihoods/likelihood.py?line=160'>161</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(function_samples, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nick_\\Anaconda3\\envs\\thesis\\lib\\site-packages\\gpytorch\\likelihoods\\softmax_likelihood.py:61\u001b[0m, in \u001b[0;36mSoftmaxLikelihood.forward\u001b[1;34m(self, function_samples, *params, **kwargs)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/nick_/Anaconda3/envs/thesis/lib/site-packages/gpytorch/likelihoods/softmax_likelihood.py?line=57'>58</a>\u001b[0m     num_data, num_features \u001b[39m=\u001b[39m function_samples\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m:]\n\u001b[0;32m     <a href='file:///c%3A/Users/nick_/Anaconda3/envs/thesis/lib/site-packages/gpytorch/likelihoods/softmax_likelihood.py?line=59'>60</a>\u001b[0m \u001b[39mif\u001b[39;00m num_features \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_features:\n\u001b[1;32m---> <a href='file:///c%3A/Users/nick_/Anaconda3/envs/thesis/lib/site-packages/gpytorch/likelihoods/softmax_likelihood.py?line=60'>61</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mThere should be \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m features\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_features)\n\u001b[0;32m     <a href='file:///c%3A/Users/nick_/Anaconda3/envs/thesis/lib/site-packages/gpytorch/likelihoods/softmax_likelihood.py?line=62'>63</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmixing_weights \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/nick_/Anaconda3/envs/thesis/lib/site-packages/gpytorch/likelihoods/softmax_likelihood.py?line=63'>64</a>\u001b[0m     mixed_fs \u001b[39m=\u001b[39m function_samples \u001b[39m@\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmixing_weights\u001b[39m.\u001b[39mt()  \u001b[39m# num_classes x num_data\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: There should be 3 features"
     ]
    }
   ],
   "source": [
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "# num_data refers to the amount of training data\n",
    "mll = VariationalELBO(likelihood, model, train_y.numel())\n",
    "\n",
    "training_iter = 5\n",
    "for i in range(training_iter):\n",
    "    # Zero backpropped gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Get predictive output\n",
    "    output = model(train_x)\n",
    "    print(output)\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, train_y)\n",
    "    loss.backward()\n",
    "    print('Iter %d/%d - Loss: %.3f' % (i + 1, training_iter, loss.item()))\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dc1a1fe9425be4de01979652791da0a8b93aa88df29b2975e2e67ada710aecb7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('thesis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
